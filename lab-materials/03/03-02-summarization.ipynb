{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Summarizing a text using an LLM\n",
    "\n",
    "As an LLM \"understands\" a language, it can be suited for tasks like translation or summarization.\n",
    "\n",
    "In this Notebook, we are going to use our LLM to summarize some texts, especially claims examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from json import load\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from certs import prepare_certs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain pipeline\n",
    "\n",
    "Again, we are going to use Langchain to define our summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server_url = \"REPLACE_ME\"\n",
    "\n",
    "prepare_certs(inference_server_url)\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=f\"{inference_server_url}/v1\",\n",
    "    model_name=\"/mnt/models/\",\n",
    "    max_tokens=512,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "The **template** we will use is now formatted for this specific summarization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text that you must summarize as best as you can.\n",
    "\n",
    "### TEXT:\n",
    "{input}\n",
    "\n",
    "### SUMMARY:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "And we can now create the **conversation** object that we will use to query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm, prompt=PROMPT, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!\n",
    "\n",
    "In the `claims` folder we have JSON files with examples of claims that could be received. We are going to read those files, display them, then the summary that the LLM made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the claims and populate a dictionary\n",
    "claims_path = 'claims'\n",
    "onlyfiles = [f for f in listdir(claims_path) if isfile(join(claims_path, f))]\n",
    "\n",
    "claims = {}\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    with open(join(claims_path, filename), 'r') as file:\n",
    "        data = load(file)\n",
    "    claims[filename] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for filename in onlyfiles:\n",
    "    print(f\"***************************\")\n",
    "    print(f\"* Claim: {filename}\")\n",
    "    print(f\"***************************\")\n",
    "    print(\"Original content:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "    print('Summary:')\n",
    "    print(\"--------\")\n",
    "    summary_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "    conversation.predict(input=summary_input);\n",
    "    print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28a5b0-6c93-42ba-84dd-42e17746d11d",
   "metadata": {},
   "source": [
    "We will come back to this notebook at section 3.5 for some exercises, so you can leave it open for the moment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
